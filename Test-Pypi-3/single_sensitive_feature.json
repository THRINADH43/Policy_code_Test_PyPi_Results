{'model': 'credit_example_test_pypi3',
 'model_version': 'v1',
 'dataset': 'credit_dataset_test_pypi3',
 'dataset_version': 'v1',
 'result': [{'complete': {'complete': {'operational': {'accuracy': 'pass',
      'precision': 'pass',
      'recall': 'fail',
      'f1': 'fail'},
     'fairness': {'balanced_accuracy': 'fail',
      'balanced_acc_error': 'fail',
      'disparate_impact': 'pass',
      'demographic_parity_difference': 'fail'},
     'operational_scores': {'accuracy': '0.7839047619047619',
      'precision': '0.8037021093413689',
      'recall': '0.5073369565217392',
      'f1': '0.6220223221722473'},
     'fairness_scores': {'balanced_accuracy': '0.7909913261638971',
      'balanced_acc_error': '0.009246206981808714',
      'disparate_impact': '1.0',
      'demographic_parity_difference': '0.06686397746375322'},
     'operational_thresholds': {'accuracy': '0.7',
      'precision': '0.8',
      'recall': '0.8',
      'f1': '0.7'},
     'fairness_thresholds': {'balanced_accuracy': '0.8',
      'balanced_acc_error': '0.7',
      'disparate_impact': '0.8',
      'demographic_parity_difference': '0.8'}}},
   'subsets': {1: {'operational': {'accuracy': 'pass',
      'precision': 'pass',
      'recall': 'fail',
      'f1': 'pass'},
     'fairness': {'balanced_accuracy': 'pass',
      'balanced_acc_error': 'fail',
      'disparate_impact': 'pass',
      'demographic_parity_difference': 'fail'},
     'operational_scores': {'accuracy': '0.8535292700554084',
      'precision': '0.8467824310520939',
      'recall': '0.6441336441336442',
      'f1': '0.7316857899382172'},
     'fairness_scores': {'balanced_accuracy': '0.8511970162826674',
      'balanced_acc_error': '0.012832841958631512',
      'disparate_impact': '1.0',
      'demographic_parity_difference': '0.0'},
     'operational_thresholds': {'accuracy': '0.7',
      'precision': '0.8',
      'recall': '0.8',
      'f1': '0.7'},
     'fairness_thresholds': {'balanced_accuracy': '0.8',
      'balanced_acc_error': '0.7',
      'disparate_impact': '0.8',
      'demographic_parity_difference': '0.8'}},
    2: {'operational': {'accuracy': 'pass',
      'precision': 'fail',
      'recall': 'fail',
      'f1': 'fail'},
     'fairness': {'balanced_accuracy': 'fail',
      'balanced_acc_error': 'fail',
      'disparate_impact': 'pass',
      'demographic_parity_difference': 'fail'},
     'operational_scores': {'accuracy': '0.7383839974799181',
      'precision': '0.7723214285714286',
      'recall': '0.43376514834935226',
      'f1': '0.5555258228525556'},
     'fairness_scores': {'balanced_accuracy': '0.7507960789210789',
      'balanced_acc_error': '0.012788216944843042',
      'disparate_impact': '1.0',
      'demographic_parity_difference': '0.0'},
     'operational_thresholds': {'accuracy': '0.7',
      'precision': '0.8',
      'recall': '0.8',
      'f1': '0.7'},
     'fairness_thresholds': {'balanced_accuracy': '0.8',
      'balanced_acc_error': '0.7',
      'disparate_impact': '0.8',
      'demographic_parity_difference': '0.8'}}}}]}